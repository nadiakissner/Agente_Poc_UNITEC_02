# Modificaci√≥n Nuevo Flujo RouteA - Resumen Ejecutivo

## üìå Cambios Realizados

### 1. **Frontend - Nuevo Componente RouteA**

**Archivo creado:** `RouteA_NUEVO_FLUJO.tsx`

**Caracter√≠sticas principales:**
- ‚úÖ Flujo conversacional totalmente nuevo (10 pasos)
- ‚úÖ Preguntas una a la vez con texto libre del usuario
- ‚úÖ Validaci√≥n de respuestas por tipo (motivaci√≥n 1-5, s√≠/no, texto)
- ‚úÖ L√≥gica condicional: Paso 2 ‚Üí Paso 3 O Paso 5 (seg√∫n tipo de duda)
- ‚úÖ Tono de escucha activa con feedback breve
- ‚úÖ Persistencia en localStorage (para recuperaci√≥n ante recarga)
- ‚úÖ Historial de chat completo
- ‚úÖ Dos salidas finales: Completado o Requiere RIASEC test

**Estructura de preguntas:**
```
Paso 1: Motivaci√≥n (1-5)
  ‚Üì
Paso 2: Tipo de duda (conmigo / carrera / ambas) [BRANCHPOINT]
  ‚îú‚Üí ambas/carrera: Paso 3
  ‚îî‚Üí conmigo: SALTA a Paso 5
Paso 3: Claridad carrera (1-5)
Paso 4: Preocupaci√≥n duraci√≥n
Paso 5: Preocupaci√≥n materias
Paso 6: Preocupaci√≥n salida laboral [+ FEEDBACK si dice S√ç]
Paso 7: Busca ayudar a otros
Paso 8: Busca demostrarse capaz
Paso 9: Busca ganar dinero
Paso 10: Decisi√≥n final (S√ç/NO/No s√©)
  ‚îú‚Üí S√ç: Finaliza ‚Üí Summary
  ‚îî‚Üí NO/No s√©: Requiere RIASEC ‚Üí routeA-riasec
```

### 2. **Backend - Funciones PHP Actualizadas**

**Archivo:** `agente-retencion-unitec-02.php`

**Funciones modificadas:**

#### a. `agente_procesar_fin_cuestionario()`
```php
// Ahora extrae:
- $motivacion_inicial (1-5 num√©rico)
- $tipo_duda (texto libre)
- $claridad_carrera (1-5)
- $duracion_concern (texto)
- $materias_concern (texto)
- $salida_laboral_concern (texto)
- $motivacion_ayudar (texto)
- $motivacion_demostrarse (texto)
- $motivacion_dinero (texto)

// Construye contexto detallado para LLM
// Guarda en BD con estructura JSON
```

#### b. `agente_clasificar_riesgo_con_llm()`
**Prompts actualizados:**

Para **'cuestionario'**:
```
Analiza el cuestionario conversacional...
Identifica:
1. Nivel motivaci√≥n inicial
2. Tipo dudas (internas vs externas)
3. Preocupaciones acad√©micas
4. Motivaciones expresadas
5. Claridad en decisi√≥n carrera

Retorna: justificacion + riesgos_identificados
```

Para **'ruta'**:
```
Analiza el impacto de la intervenci√≥n...
Determina:
1. Cambio en claridad/confianza
2. Cambio en motivaci√≥n
3. Recomendaciones seguimiento
4. Prioridad intervenci√≥n

Retorna: justificacion + prioridad_sugerida
```

### 3. **Base de Datos - Estructura Utilizada**

La tabla `byw_agente_retencion` recibe:

```json
{
  "ID": 123,
  "user_id": 456,
  "user_email": "estudiante@unitec.edu",
  "riesgo_detectado": ["duda_interna", "preocupacion_academica"],
  "prioridad_caso": "alto|medio|bajo|pendiente",
  "justificacion": {
    "cuestionario": "An√°lisis de respuestas conversacionales...",
    "ruta": "An√°lisis de impacto de intervenci√≥n..."
  },
  "ultima_actividad": "2025-01-15 10:30:00"
}
```

### 4. **Flujo de Datos Completo**

```
Usuario en Frontend
    ‚Üì
Responde Paso 1 (motivaci√≥n 1-5)
    ‚Üì
Responde Paso 2 (tipo duda) ‚Üí L√ìGICA CONDICIONAL
    ‚îú‚Üí ambas ‚Üí Contin√∫a Paso 3
    ‚îî‚Üí conmigo ‚Üí SALTA a Paso 5
    ‚Üì
Responde Pasos 3-10 secuencialmente
    ‚Üì
Historial de chat se acumula
    ‚Üì
Respuestas se guardan en objeto
    ‚Üì
Usuario decide en Paso 10
    ‚Üì
Frontend env√≠a POST /procesar-fin-cuestionario
    ‚îú‚îÄ Payload: respuestas (1-10), historial chat, conversacion completa
    ‚îî‚îÄ Body: user_id, user_email, nombre, carrera, respuestas, conversacion, status
    ‚Üì
Backend extrae informaci√≥n contextual
    ‚Üì
Backend llama LLM con PROMPT nuevo
    ‚Üì
LLM retorna: justificacion + riesgos_identificados
    ‚Üì
Backend guarda en byw_agente_retencion:
    - prioridad_caso = 'pendiente' (se actualiza despu√©s en ruta)
    - justificacion = JSON con an√°lisis
    - riesgo_detectado = array de riesgos
    ‚Üì
Frontend redirige:
    ‚îú‚Üí Si S√ç: /summary (fin del flujo)
    ‚îî‚Üí Si NO: /routeA-riasec (contin√∫a con test)
```

## üîç Cambios T√©cnicos Clave

| Aspecto | Antes | Ahora |
|--------|--------|--------|
| **Tipo de respuesta** | Selecci√≥n m√∫ltiple (Chips) | Texto libre del usuario |
| **N√∫mero de pasos** | Flexible (8-12 seg√∫n rama) | Fijo: 10 pasos m√°ximo |
| **L√≥gica de bifurcaci√≥n** | En m√∫ltiples puntos | Solo en Paso 2 |
| **Almacenamiento respuestas** | Variables separadas | Objeto indexado por step |
| **An√°lisis LLM** | Respuestas estructuradas | Conversaci√≥n + respuestas |
| **Historial guardado** | No persistido | Completo en localStorage + BD |
| **Validaci√≥n** | UI solo | UI + Backend |
| **Feedback usuario** | √önico al final | Incrementales (ej: Paso 6) |

## ‚úÖ Implementaci√≥n Checklist

### Frontend
- [ ] Reemplazar `RouteA.tsx` con contenido de `RouteA_NUEVO_FLUJO.tsx`
- [ ] Probar flujo paso a paso (1 ‚Üí 10)
- [ ] Probar l√≥gica condicional Paso 2 (ambas ‚Üí 3 vs conmigo ‚Üí 5)
- [ ] Probar persistencia localStorage (recarga p√°gina)
- [ ] Probar validaci√≥n motivaci√≥n (rechaza valores > 5)
- [ ] Probar endpoint POST al completar

### Backend
- [ ] Actualizar `agente_procesar_fin_cuestionario()` con nuevo contexto
- [ ] Actualizar prompts en `agente_clasificar_riesgo_con_llm()`
- [ ] Verificar extracci√≥n correcta de respuestas conversacionales
- [ ] Probar llamada a OpenAI con nuevo prompt
- [ ] Verificar guardado en `byw_agente_retencion` con estructura correcta

### Integration
- [ ] Probar flujo END-TO-END: Frontend ‚Üí Backend ‚Üí LLM ‚Üí BD
- [ ] Verificar que LLM interpreta respuestas conversacionales
- [ ] Verificar que prioridad se calcula correctamente
- [ ] Probar ambas salidas (S√ç ‚Üí summary, NO ‚Üí riasec)

## üìö Documentaci√≥n Relacionada

- [GUIA_NUEVO_FLUJO_ROUTEA.md](GUIA_NUEVO_FLUJO_ROUTEA.md) - Detalle t√©cnico completo
- [RouteA_NUEVO_FLUJO.tsx](./SRC/Pages/RouteA_NUEVO_FLUJO.tsx) - C√≥digo fuente frontend
- [NOTA_ESTRUCTURA_TABLA.md](NOTA_ESTRUCTURA_TABLA.md) - Estructura BD

## üöÄ Pr√≥ximas Fases

1. **Fase 1 (Actualizaci√≥n):** Implementar nuevo RouteA
2. **Fase 2 (RIASEC):** Crear `/routeA-riasec` para rama "No" del paso 10
3. **Fase 3 (Validaci√≥n):** Testing end-to-end con usuarios reales
4. **Fase 4 (Iteraci√≥n):** Refinar prompts del LLM seg√∫n resultados reales

## ‚ö†Ô∏è Notas Importantes

- **Compatibilidad:** Nuevo flujo es 100% independiente del antiguo RouteA
- **Persistencia:** Todas las respuestas se persisten en localStorage y BD
- **Recuperaci√≥n:** Si usuario recarga p√°gina, puede continuar desde donde estaba
- **LLM:** Los prompts ahora interpretan conversaci√≥n COMPLETA, no solo respuestas
- **Validaci√≥n:** Las respuestas de "motivaci√≥n" se validan como n√∫meros 1-5

---

**Versi√≥n:** 1.0 (Enero 2025)
**Estado:** Listo para implementaci√≥n
